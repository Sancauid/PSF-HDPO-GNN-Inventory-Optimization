# File: experiments/S6_OWMS_Lost_5S_8192Scenarios_GNN.yml

# 1. METADATA & EXPERIMENT SETUP
experiment:
  name: "S6_OWMS_Lost_5S_8192Scenarios_GNN"
  setting_id: "S6"
  seed: 57 # From 'demand' seed in old config
  device: "cuda"

# 2. ENVIRONMENT DEFINITION
environment:
  topology:
    generator: "from_rules"
    rules: { node_counts: { warehouse: 1, store: 5 }, edge_policy: "warehouse_to_all_stores" }
  demand:
    source: "synthetic"
    synthetic_data_config:
      distribution: "normal"
      mean_range: [2.5, 7.5]
      cv_range: [0.25, 0.5]
      correlation: 0.5
  parameters:
    nodes:
      stores:
        holding_cost: { sampling_method: "uniform", range: [0.7, 1.3] }
        underage_cost: { sampling_method: "uniform", range: [6.3, 11.7] }
      warehouses:
        holding_cost: { sampling_method: "fixed", value: 0.3 }
    edges:
      supplier_to_warehouse: { lead_time: { sampling_method: "fixed", value: 3 } }
      warehouse_to_store: { lead_time: { sampling_method: "uniform_int", range: [2, 5] } }
  dynamics:
    unmet_demand_assumption: "lost"
    is_transshipment: false
    objective: "minimize_cost"

# 3. FEATURE ENGINEERING
features:
  dynamic: ["inventory_on_hand", "outstanding_orders"]
  static: ["holding_cost", "underage_cost", "lead_time", "demand_mean", "demand_std"]
  demand_history: { past_periods: 0 }
  time_features: null

# 4. DATA HANDLING
data:
  split_policy: "random"
  scenarios: { train: 8192, dev: 8192, test: 8192 }
  episode_length: { train: 50, dev: 100, test: 5000 }
  warmup_periods: { train: 30, dev: 60, test: 3000 }
  batch_size: 1024

# 5. MODEL ARCHITECTURE (from gnn.yml)
model:
  architecture_class: "gnn"
  gnn_params:
    fel_type: "g1b"
    message_passing_layers: 2
    weight_sharing: true
    module_defaults:
      layers: 2
      width: 32
      activation: "elu"
    modules:
      Readout: { output_activation: "softplus" }

# 6. TRAINING PROCESS (from gnn.yml)
training:
  optimizer:
    name: "adam"
    lr: 0.001 # From optimizer_params
  epochs: 20000 # From trainer_params
  early_stopping_patience: 500 # From trainer_params
  gradient_clip_norm: 1.0 # From nn_params
  dev_eval_freq_epochs: 1 # From trainer_params
  print_log_freq_epochs: 1
  save_best_model: false